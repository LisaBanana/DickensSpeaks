{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open('corpus.txt','rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 25439 characters\n"
     ]
    }
   ],
   "source": [
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One winter's evening, towards the close of the year 1800, or within a year\n",
      "or two of that time, a young medical practitioner, recently established in\n",
      "business, was seated by a cheerful fire in his little parlour, listening to the\n",
      "wind which was be\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  \"'\" :   4,\n",
      "  '(' :   5,\n",
      "  ')' :   6,\n",
      "  ',' :   7,\n",
      "  '-' :   8,\n",
      "  '.' :   9,\n",
      "  '0' :  10,\n",
      "  '1' :  11,\n",
      "  '8' :  12,\n",
      "  ':' :  13,\n",
      "  ';' :  14,\n",
      "  '?' :  15,\n",
      "  'A' :  16,\n",
      "  'B' :  17,\n",
      "  'C' :  18,\n",
      "  'D' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"One winter's \" ---- characters mapped to int ---- > [29 52 43  2 61 47 52 58 43 56  4 57  2]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "n\n",
      "e\n",
      " \n",
      "w\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"One winter's evening, towards the close of the year 1800, or within a year\\r\\nor two of that time, a yo\"\n",
      "'ung medical practitioner, recently established in\\r\\nbusiness, was seated by a cheerful fire in his lit'\n",
      "'tle parlour, listening to the\\r\\nwind which was beating the rain in pattering drops against the window,'\n",
      "' or\\r\\nrumbling dismally in the chimney. The night was wet and cold; he had\\r\\nbeen walking through mud a'\n",
      "'nd water the whole day, and was now\\r\\ncomfortably reposing in his dressing-gown and slippers, more tha'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"One winter's evening, towards the close of the year 1800, or within a year\\r\\nor two of that time, a y\"\n",
      "Target data: \"ne winter's evening, towards the close of the year 1800, or within a year\\r\\nor two of that time, a yo\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 29 ('O')\n",
      "  expected output: 52 ('n')\n",
      "Step    1\n",
      "  input: 52 ('n')\n",
      "  expected output: 43 ('e')\n",
      "Step    2\n",
      "  input: 43 ('e')\n",
      "  expected output: 2 (' ')\n",
      "Step    3\n",
      "  input: 2 (' ')\n",
      "  expected output: 61 ('w')\n",
      "Step    4\n",
      "  input: 61 ('w')\n",
      "  expected output: 47 ('i')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 62, 60, 27,  6, 27, 64, 17, 53,  8,  5, 63, 33, 38, 16, 18, 11,\n",
       "       37,  1, 38, 62, 36, 44, 61, 25, 21, 46, 20, 11, 46, 35, 47, 57, 58,\n",
       "       59, 14, 46, 54, 19, 63,  0,  3, 19, 36, 19, 60, 52, 57, 51, 28, 61,\n",
       "       18,  8,  0, 49, 17, 51, 34, 63,  1,  8, 63, 17, 17, 12, 51, 30,  1,\n",
       "       41, 41,  6, 59,  6, 51,  2, 64, 54, 11, 56, 33, 24, 60, 46, 41,  8,\n",
       "        1, 43,  6, 10, 16,  7, 15, 43, 37, 14, 36,  3,  2, 36, 19])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " '; and, now and then, a miserable patch of garden-ground,\\r\\nwith a few old boards knocked together for'\n",
      "\n",
      "Next Char Predictions: \n",
      " ')xvM)MzBo-(yT_AC1Y\\r_xWfwKFhE1hVistu;hpDy\\n!DWDvnsmNwC-\\nkBmUy\\r-yBB8mP\\rcc)u)m zp1rTIvhc-\\re)0A,?eY;W! WD'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.1737595\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 3 steps\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.1229\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.7004\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.6706\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.6113\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.4851\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.3605\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.1765\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.0148\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.0000\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.9265\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.8678\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.8282\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.7618\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.7032\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.6463\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.5814\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.5307\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.4905\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.4659\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.4188\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.4072\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.3888\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.3594\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.3371\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.3150\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.3006\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.2873\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.2723\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.2504\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.2471\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.2362\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.2193\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.2160\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.1977\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.1830\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.1765\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.1648\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.1538\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.1447\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.1291\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.1230\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.1063\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.0907\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.0796\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.0872\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.0582\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.0529\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.0433\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.0308\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.0107\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_50'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 5\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is caig\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"The sky is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(inp)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            target, predictions, from_logits=True))\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.175134658813477\n",
      "Epoch 1 Loss 4.0693\n",
      "Time taken for 1 epoch 6.118037939071655 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.8320510387420654\n",
      "Epoch 2 Loss 3.5987\n",
      "Time taken for 1 epoch 5.000967025756836 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.8342347145080566\n",
      "Epoch 3 Loss 3.9507\n",
      "Time taken for 1 epoch 5.501431703567505 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 3.955986261367798\n",
      "Epoch 4 Loss 3.9279\n",
      "Time taken for 1 epoch 5.202840566635132 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.906343936920166\n",
      "Epoch 5 Loss 3.8335\n",
      "Time taken for 1 epoch 5.206571102142334 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 3.7908694744110107\n",
      "Epoch 6 Loss 3.6584\n",
      "Time taken for 1 epoch 5.032634735107422 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 3.572174072265625\n",
      "Epoch 7 Loss 3.3720\n",
      "Time taken for 1 epoch 5.031881809234619 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 3.3343067169189453\n",
      "Epoch 8 Loss 3.1891\n",
      "Time taken for 1 epoch 5.054827928543091 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 3.074802875518799\n",
      "Epoch 9 Loss 2.9634\n",
      "Time taken for 1 epoch 5.0290751457214355 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 3.0132524967193604\n",
      "Epoch 10 Loss 2.9682\n",
      "Time taken for 1 epoch 5.139891147613525 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # initializing the hidden state at the start of every epoch\n",
    "  # initally hidden is None\n",
    "  hidden = model.reset_states()\n",
    "\n",
    "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "    loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "      template = 'Epoch {} Batch {} Loss {}'\n",
    "      print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "  # saving (checkpoint) the model every 5 epochs\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
